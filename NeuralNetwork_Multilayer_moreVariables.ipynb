{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with all variables\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the variables that are not considered and do not impact the correlation\n",
    "BSOM_temp=BSOM_data.drop(columns=['Random_ID', 'LEVEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 20 variables strongly correlated with 'STEP_1'\n",
    "BSOM_corr = BSOM_temp.corr()\n",
    "print (BSOM_corr['STEP_1'].sort_values(ascending=False)[:20], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix with few of the above selected variables\n",
    "BSOM_corr_features=pd.read_csv('BSOM_DataSet_for_HW3.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','BCR_NBME_final','B2E_MCQ4_IND','CBSE_02','B2E_NBME_final','B2E_MCQ_AVG_04','B2E_PI_AVG_30','B2E_MCQ4_TOT','O1_O2_NBME','B2E_MCQ4_IND','SA_MCQ_AVG_05','SA_NBME',\n",
    "                                                                    'BCR_MCQ_AVG_04','BCR_PI_AVG_30','O1O2_PI_AVG_26'])\n",
    "corr_features=BSOM_corr_features.corr()\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_features,cmap='coolwarm',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to variables\n",
    "def Feature_scaling(feat):\n",
    "    num=feat.shape[0]\n",
    "    mean_X=np.mean(feat,axis=1)\n",
    "    range_X=(np.amax(feat, axis=1)-np.amin(feat, axis=1))\n",
    "    for i in range(1,num):\n",
    "        xi=feat[i,:]\n",
    "        xi=(xi-mean_X[i])/range_X[i]\n",
    "        feat[i,:]=xi\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly initialize the parameters\n",
    "#Note: bias thetas are initialized separately\n",
    "#Takes the list of number of nodes in each layer as its elements, as input and returns the number of layers and \n",
    "#   randomly initialised parameters in a dictionary\n",
    "def initialize_params(nodes_list):\n",
    "    np.random.seed(1)\n",
    "    e=0.0001\n",
    "    num_layers=len(nodes_list)\n",
    "    thetasandbias={}\n",
    "    for i in range(1,num_layers):\n",
    "        thetasandbias['theta_L'+str(i)]= (np.random.randn(nodes_list[i],nodes_list[i-1]))*e\n",
    "        thetasandbias['bias_L'+str(i)]=(np.random.randn(nodes_list[i],1))*e\n",
    "    return thetasandbias,num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    a = 1/(1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of sigmoid function (used in back propagation)\n",
    "def d_sigmoid(a):\n",
    "    d_s = a*(1-a)\n",
    "    return d_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation \n",
    "#returns the activation functions of all the layers in a dictionary\n",
    "def forward_prop(X,thetas,L):\n",
    "    activations={}\n",
    "    activations['a_L1']=X\n",
    "    for i in range(1,L):\n",
    "        z=np.dot(thetas['theta_L'+str(i)],activations['a_L'+str(i)])+thetas['bias_L'+str(i)]\n",
    "        a=sigmoid(z)\n",
    "        activations['a_L'+str(i+1)]=a\n",
    "    h=activations['a_L'+str(L)]\n",
    "    return h,activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate cost function for ridge regularization\n",
    "def calc_cost_reg(ypred,yactual,thetas,L,lamda):\n",
    "    m=yactual.shape[1]\n",
    "    cost=(-1/m)*(np.sum(np.sum((np.multiply(yactual,np.log(ypred))+np.multiply((1-yactual),np.log(1-ypred))),axis=0)))\n",
    "    theta_sqr_sum=0\n",
    "    for i in range(1,L):\n",
    "        theta_sqr_sum+=np.sum(np.square(thetas['theta_L'+str(i)]))\n",
    "        \n",
    "    #theta_sqr_sum=np.sum(np.square(thetas['theta_L1']))+np.sum(np.square(thetas['theta_L2']))\n",
    "    reg_cost=(lamda/(2*m))*theta_sqr_sum\n",
    "    final_cost=cost+reg_cost\n",
    "    return final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward propagation with ridge regularization\n",
    "#returns the partial derivatives of thetas of all the layers(both for non bias ans bias separately) in a dictionary\n",
    "def backward_prop_reg(thetas,X,activation,y_actual,L,lamda):\n",
    "    m=X.shape[1]\n",
    "    deltas={}\n",
    "    d_thetas={}\n",
    "    delta_L=activation['a_L'+str(L)]-y_actual\n",
    "    deltas['delta_L'+str(L)]=delta_L\n",
    "    for i in range(L,2,-1):\n",
    "        d_theta_Lprev=(1/m)*np.dot(deltas['delta_L'+str(i)],activation['a_L'+str(i-1)].T)+((lamda/m)*(thetas['theta_L'+str(i-1)]))\n",
    "        d_bias_Lprev=(1/m)*np.sum(deltas['delta_L'+str(i)],axis=1,keepdims=True)\n",
    "        deltas['delta_L'+str(i-1)]=np.multiply(np.dot(thetas['theta_L'+str(i-1)].T,deltas['delta_L'+str(i)]),d_sigmoid(activation['a_L'+str(i-1)]))\n",
    "        d_thetas['d_theta_L'+str(i-1)]=d_theta_Lprev\n",
    "        d_thetas['d_bias_L'+str(i-1)]=d_bias_Lprev\n",
    "        \n",
    "    d_theta_L1 = (1/m)*np.dot(deltas['delta_L2'],X.T)+((lamda/m)*(thetas['theta_L1']))\n",
    "    d_bias_L1=(1/m)*np.sum(deltas['delta_L2'],axis=1,keepdims=True)\n",
    "    d_thetas['d_theta_L1']=d_theta_L1\n",
    "    d_thetas['d_bias_L1']=d_bias_L1\n",
    "    \n",
    "\n",
    "    return d_thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the thetas of all the layers\n",
    "def update_thetas(thetas,d_thetas,L,alpha):\n",
    "    thetas_updated=thetas\n",
    "    for i in range(1,L):\n",
    "        thetas_updated['theta_L'+str(i)]=thetas['theta_L'+str(i)]-(alpha*d_thetas['d_theta_L'+str(i)])\n",
    "        thetas_updated['bias_L'+str(i)]=thetas['bias_L'+str(i)]-(alpha*d_thetas['d_bias_L'+str(i)])\n",
    "    return thetas_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_multihiddenlayer_Reg(X,y,nodelist,num_iter,alpha,lamda):\n",
    "    thetas_reg,num_layer = initialize_params(nodelist)\n",
    "    count=0\n",
    "    cost_list_reg=[]\n",
    "    while count<num_iter:\n",
    "        ypred,act_function=forward_prop(X,thetas_reg,num_layer)\n",
    "        cost = calc_cost_reg(ypred,y,thetas_reg,num_layer,lamda)\n",
    "        cost_list_reg.append(cost)\n",
    "        pd_thetas = backward_prop_reg(thetas_reg,X,act_function,y,num_layer,lamda)\n",
    "        if (len(cost_list_reg)>=2) and ((cost_list_reg[count-1]-cost_list_reg[count])<0.0000001):\n",
    "            print(\"convergence is reached at iteration\\n\",str(count))\n",
    "            break\n",
    "        thetas_reg=update_thetas(thetas_reg,pd_thetas,num_layer,alpha)\n",
    "        count+=1\n",
    "    return cost_list_reg,thetas_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the cost (vs) iterations graph\n",
    "def plot_costfunction(iter_num,J_list,data):\n",
    "    iterations=list(np.arange(0,iter_num,1))\n",
    "    cost_J=[]\n",
    "    for i in iterations:\n",
    "        cost_J.append(J_list[i])\n",
    "\n",
    "    plt.plot(iterations,cost_J)\n",
    "    plt.xlabel(\"#Iterations\")\n",
    "    plt.ylabel(\"J (cost)\")\n",
    "    plt.title(\"NeuralNetworks cost function vs iterations \" +str(data))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "    sns.heatmap(cf,xticklabels=['A','B','C','D'],yticklabels=['A','B','C','D'],annot=True,linecolor='white',linewidths=0.5,cmap='coolwarm')\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"actual labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with 5 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with 5 variables\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW3.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','O1O2_PI_AVG_26','CBSE_01','CBSE_02','LEVEL'])\n",
    "#checking for missing values\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows with missing values\n",
    "BSOM_data=BSOM_data.dropna(axis=0)\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(80%) and test(20%) datasets\n",
    "features_X = BSOM_data.iloc[:,:-1].to_numpy()\n",
    "y=BSOM_data.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(features_X, y, test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to the independent variables and one hot encoding the target variable\n",
    "train_X = Xtrain.T\n",
    "m_train=train_X.shape[0]\n",
    "train_X=Feature_scaling(train_X)\n",
    "train_y=ytrain\n",
    "train_y=pd.get_dummies(train_y).to_numpy()\n",
    "train_y=train_y.T\n",
    "test_X = Xtest.T\n",
    "m_test=test_X.shape[0]\n",
    "test_X=Feature_scaling(test_X)\n",
    "test_y=ytest\n",
    "test_y=pd.get_dummies(test_y).to_numpy()\n",
    "test_y=test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the class labels in the train data\n",
    "actual_train=ytrain\n",
    "actual_train=np.where(actual_train=='A', 0, actual_train)\n",
    "actual_train=np.where(actual_train=='B', 1, actual_train)\n",
    "actual_train=np.where(actual_train=='C', 2, actual_train)\n",
    "actual_train=np.where(actual_train=='D', 3, actual_train)\n",
    "#encode the class labels in the test data\n",
    "actual_test=ytest\n",
    "actual_test=np.where(actual_test=='A', 0, actual_test)\n",
    "actual_test=np.where(actual_test=='B', 1, actual_test)\n",
    "actual_test=np.where(actual_test=='C', 2, actual_test)\n",
    "actual_test=np.where(actual_test=='D', 3, actual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model with ridge regularization\n",
    "iter_num=1500\n",
    "alpha=0.7\n",
    "lamda=0.1\n",
    "layernodes=[5,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"lamda :\\n\",str(lamda))\n",
    "final_cost_reg,final_thetas_reg=NN_multihiddenlayer_Reg(train_X,train_y,layernodes,iter_num,alpha,lamda)\n",
    "ypred_train,activations_train=forward_prop(train_X,final_thetas_reg,n_layers)\n",
    "ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "print(\"Confusion Matrix of training data\\n\")\n",
    "cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "print(cf)\n",
    "pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "print(\"Precision : \",str(pr))\n",
    "print(\"Recall : \",str(rc))\n",
    "print(\"F1 score : \",str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model with ridge regularization\n",
    "alpha=0.7\n",
    "lamda=0.1\n",
    "layernodes=[5,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"lamda :\\n\",str(lamda))\n",
    "ypred_test,activations_test=forward_prop(test_X,final_thetas_reg,n_layers)\n",
    "ypred_labels_test=np.argmax(ypred_test,axis=0)\n",
    "# print(ypred_labels_test)\n",
    "# print(actual_test)\n",
    "print(\"Confusion Matrix of test data\\n\")\n",
    "cf_test=confusion_matrix(list(actual_test),list(ypred_labels_test))\n",
    "print(cf_test)\n",
    "pr_test=precision_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "rc_test=recall_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "f1_test=f1_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "print(\"Precision : \",str(pr_test))\n",
    "print(\"Recall : \",str(rc_test))\n",
    "print(\"F1 score : \",str(f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with 6 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with 5 variables\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW3.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','O1O2_PI_AVG_26','B2E_MCQ_AVG_04','CBSE_01','CBSE_02','LEVEL'])\n",
    "#checking for missing values\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows with missing values\n",
    "BSOM_data=BSOM_data.dropna(axis=0)\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(80%) and test(20%) datasets\n",
    "features_X = BSOM_data.iloc[:,:-1].to_numpy()\n",
    "y=BSOM_data.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(features_X, y, test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to the independent variables and one hot encoding the target variable\n",
    "train_X = Xtrain.T\n",
    "m_train=train_X.shape[0]\n",
    "train_X=Feature_scaling(train_X)\n",
    "train_y=ytrain\n",
    "train_y=pd.get_dummies(train_y).to_numpy()\n",
    "train_y=train_y.T\n",
    "test_X = Xtest.T\n",
    "m_test=test_X.shape[0]\n",
    "test_X=Feature_scaling(test_X)\n",
    "test_y=ytest\n",
    "test_y=pd.get_dummies(test_y).to_numpy()\n",
    "test_y=test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the class labels in the train data\n",
    "actual_train=ytrain\n",
    "actual_train=np.where(actual_train=='A', 0, actual_train)\n",
    "actual_train=np.where(actual_train=='B', 1, actual_train)\n",
    "actual_train=np.where(actual_train=='C', 2, actual_train)\n",
    "actual_train=np.where(actual_train=='D', 3, actual_train)\n",
    "#encode the class labels in the test data\n",
    "actual_test=ytest\n",
    "actual_test=np.where(actual_test=='A', 0, actual_test)\n",
    "actual_test=np.where(actual_test=='B', 1, actual_test)\n",
    "actual_test=np.where(actual_test=='C', 2, actual_test)\n",
    "actual_test=np.where(actual_test=='D', 3, actual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model with ridge regularization\n",
    "iter_num=1500\n",
    "alpha=0.7\n",
    "lamda=0.1\n",
    "layernodes=[6,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"lamda :\\n\",str(lamda))\n",
    "final_cost_reg,final_thetas_reg=NN_multihiddenlayer_Reg(train_X,train_y,layernodes,iter_num,alpha,lamda)\n",
    "ypred_train,activations_train=forward_prop(train_X,final_thetas_reg,n_layers)\n",
    "ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "print(\"Confusion Matrix of training data\\n\")\n",
    "cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "print(cf)\n",
    "pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "print(\"Precision : \",str(pr))\n",
    "print(\"Recall : \",str(rc))\n",
    "print(\"F1 score : \",str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model with ridge regularization\n",
    "alpha=0.7\n",
    "lamda=0.1\n",
    "layernodes=[6,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"lamda :\\n\",str(lamda))\n",
    "ypred_test,activations_test=forward_prop(test_X,final_thetas_reg,n_layers)\n",
    "ypred_labels_test=np.argmax(ypred_test,axis=0)\n",
    "# print(ypred_labels_test)\n",
    "# print(actual_test)\n",
    "print(\"Confusion Matrix of test data\\n\")\n",
    "cf_test=confusion_matrix(list(actual_test),list(ypred_labels_test))\n",
    "print(cf_test)\n",
    "pr_test=precision_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "rc_test=recall_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "f1_test=f1_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "print(\"Precision : \",str(pr_test))\n",
    "print(\"Recall : \",str(rc_test))\n",
    "print(\"F1 score : \",str(f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

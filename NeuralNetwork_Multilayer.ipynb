{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with 4 variables\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW3.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','CBSE_01','CBSE_02','LEVEL'])\n",
    "#checking for missing values\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows with missing values\n",
    "BSOM_data=BSOM_data.dropna(axis=0)\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to variables\n",
    "def Feature_scaling(feat):\n",
    "    num=feat.shape[0]\n",
    "    mean_X=np.mean(feat,axis=1)\n",
    "    range_X=(np.amax(feat, axis=1)-np.amin(feat, axis=1))\n",
    "    for i in range(1,num):\n",
    "        xi=feat[i,:]\n",
    "        xi=(xi-mean_X[i])/range_X[i]\n",
    "        feat[i,:]=xi\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly initialize the parameters\n",
    "#Note: bias thetas are initialized separately\n",
    "#Takes the list of number of nodes in each layer as its elements, as input and returns the number of layers and \n",
    "#   randomly initialised parameters in a dictionary\n",
    "def initialize_params(nodes_list):\n",
    "    np.random.seed(1)\n",
    "    e=0.0001\n",
    "    num_layers=len(nodes_list)\n",
    "    thetasandbias={}\n",
    "    for i in range(1,num_layers):\n",
    "        thetasandbias['theta_L'+str(i)]= (np.random.randn(nodes_list[i],nodes_list[i-1]))*e\n",
    "        thetasandbias['bias_L'+str(i)]=(np.random.randn(nodes_list[i],1))*e\n",
    "    return thetasandbias,num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    a = 1/(1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of sigmoid function (used in back propagation)\n",
    "def d_sigmoid(a):\n",
    "    d_s = a*(1-a)\n",
    "    return d_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation \n",
    "#returns the activation functions of all the layers in a dictionary\n",
    "def forward_prop(X,thetas,L):\n",
    "    activations={}\n",
    "    activations['a_L1']=X\n",
    "    for i in range(1,L):\n",
    "        z=np.dot(thetas['theta_L'+str(i)],activations['a_L'+str(i)])+thetas['bias_L'+str(i)]\n",
    "        a=sigmoid(z)\n",
    "        activations['a_L'+str(i+1)]=a\n",
    "    h=activations['a_L'+str(L)]\n",
    "    return h,activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the cost function\n",
    "def calc_cost(ypred,yactual):\n",
    "    m=yactual.shape[1]\n",
    "    cost=(-1/m)*(np.sum(np.sum((np.multiply(yactual,np.log(ypred))+np.multiply((1-yactual),np.log(1-ypred))),axis=0)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward propagation\n",
    "#returns the partial derivatives of thetas of all the layers(both for non bias ans bias separately) in a dictionary\n",
    "def backward_prop(thetas,X,activation,y_actual,L):\n",
    "    m=X.shape[1]\n",
    "    deltas={}\n",
    "    d_thetas={}\n",
    "    delta_L=activation['a_L'+str(L)]-y_actual\n",
    "    deltas['delta_L'+str(L)]=delta_L\n",
    "    for i in range(L,2,-1):\n",
    "        d_theta_Lprev=(1/m)*np.dot(deltas['delta_L'+str(i)],activation['a_L'+str(i-1)].T)\n",
    "        d_bias_Lprev=(1/m)*np.sum(deltas['delta_L'+str(i)],axis=1,keepdims=True)\n",
    "        deltas['delta_L'+str(i-1)]=np.multiply(np.dot(thetas['theta_L'+str(i-1)].T,deltas['delta_L'+str(i)]),d_sigmoid(activation['a_L'+str(i-1)]))\n",
    "        d_thetas['d_theta_L'+str(i-1)]=d_theta_Lprev\n",
    "        d_thetas['d_bias_L'+str(i-1)]=d_bias_Lprev\n",
    "        \n",
    "    d_theta_L1 = (1/m)*np.dot(deltas['delta_L2'],X.T)\n",
    "    d_bias_L1=(1/m)*np.sum(deltas['delta_L2'],axis=1,keepdims=True)\n",
    "    d_thetas['d_theta_L1']=d_theta_L1\n",
    "    d_thetas['d_bias_L1']=d_bias_L1\n",
    "    \n",
    "\n",
    "    return d_thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the thetas of all the layers\n",
    "def update_thetas(thetas,d_thetas,L,alpha):\n",
    "    thetas_updated=thetas\n",
    "    for i in range(1,L):\n",
    "        thetas_updated['theta_L'+str(i)]=thetas['theta_L'+str(i)]-(alpha*d_thetas['d_theta_L'+str(i)])\n",
    "        thetas_updated['bias_L'+str(i)]=thetas['bias_L'+str(i)]-(alpha*d_thetas['d_bias_L'+str(i)])\n",
    "    return thetas_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_multihiddenlayer(X,y,nodelist,num_iter,alpha):\n",
    "    thetas,num_layer = initialize_params(nodelist)\n",
    "    count=0\n",
    "    cost_list=[]\n",
    "    while count<num_iter:\n",
    "        ypred,act_function=forward_prop(X,thetas,num_layer)\n",
    "        cost = calc_cost(ypred,y)\n",
    "        cost_list.append(cost)\n",
    "        pd_thetas = backward_prop(thetas,X,act_function,y,num_layer)\n",
    "        if (len(cost_list)>=2) and ((cost_list[count-1]-cost_list[count])<0.0000001):\n",
    "            print(\"convergence is reached at iteration\\n\",str(count))\n",
    "            break\n",
    "        thetas=update_thetas(thetas,pd_thetas,num_layer,alpha)\n",
    "        count+=1\n",
    "    return cost_list,thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the cost (vs) iterations graph\n",
    "def plot_costfunction(iter_num,J_list,data):\n",
    "    iterations=list(np.arange(0,iter_num,1))\n",
    "    cost_J=[]\n",
    "    for i in iterations:\n",
    "        cost_J.append(J_list[i])\n",
    "\n",
    "    plt.plot(iterations,cost_J)\n",
    "    plt.xlabel(\"#Iterations\")\n",
    "    plt.ylabel(\"J (cost)\")\n",
    "    plt.title(\"NeuralNetworks cost function vs iterations \" +str(data))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "    sns.heatmap(cf,xticklabels=['A','B','C','D'],yticklabels=['A','B','C','D'],annot=True,linecolor='white',linewidths=0.5,cmap='coolwarm')\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"actual labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate AUC score for multiclass classification\n",
    "def calc_AUCscore(actual, predicted):\n",
    "    label_bin=LabelBinarizer()\n",
    "    label_bin.fit(actual)\n",
    "    actual = label_bin.transform(actual)\n",
    "    predicted = label_bin.transform(predicted)\n",
    "    return roc_auc_score(actual, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(80%) and test(20%) datasets\n",
    "features_X = BSOM_data.iloc[:,:-1].to_numpy()\n",
    "y=BSOM_data.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(features_X, y, test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to the independent variables and one hot encoding the target variable\n",
    "train_X = Xtrain.T\n",
    "m_train=train_X.shape[0]\n",
    "train_X=Feature_scaling(train_X)\n",
    "train_y=ytrain\n",
    "train_y=pd.get_dummies(train_y).to_numpy()\n",
    "train_y=train_y.T\n",
    "test_X = Xtest.T\n",
    "m_test=test_X.shape[0]\n",
    "test_X=Feature_scaling(test_X)\n",
    "test_y=ytest\n",
    "test_y=pd.get_dummies(test_y).to_numpy()\n",
    "test_y=test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the class labels in the train data\n",
    "actual_train=ytrain\n",
    "actual_train=np.where(actual_train=='A', 0, actual_train)\n",
    "actual_train=np.where(actual_train=='B', 1, actual_train)\n",
    "actual_train=np.where(actual_train=='C', 2, actual_train)\n",
    "actual_train=np.where(actual_train=='D', 3, actual_train)\n",
    "#encode the class labels in the test data\n",
    "actual_test=ytest\n",
    "actual_test=np.where(actual_test=='A', 0, actual_test)\n",
    "actual_test=np.where(actual_test=='B', 1, actual_test)\n",
    "actual_test=np.where(actual_test=='C', 2, actual_test)\n",
    "actual_test=np.where(actual_test=='D', 3, actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model with single hidden layer with 5 hidden nodes using the train dataset (comparing different learning rate values)\n",
    "iter_num=1500\n",
    "alphas_list=[0.01,0.1,0.5,0.6,0.7]\n",
    "layernodes=[4,5,4]\n",
    "n_layers=len(layernodes)\n",
    "for i in alphas_list:\n",
    "    print(\"learning_rate :\\n\",str(i))\n",
    "    final_cost,final_thetas=NN_multihiddenlayer(train_X,train_y,layernodes,iter_num,i)\n",
    "    ypred_train,activations_train=forward_prop(train_X,final_thetas,n_layers)\n",
    "    ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "    print(cf)\n",
    "    pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    print(\"Precision : \",str(pr))\n",
    "    print(\"Recall : \",str(rc))\n",
    "    print(\"F1 score : \",str(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model with single hidden layer with 5 hidden nodes using the train dataset (with selected best learning rate  )\n",
    "iter_num=1500\n",
    "alpha=0.7\n",
    "layernodes=[4,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"learning_rate :\\n\",str(alpha))\n",
    "final_cost,final_thetas=NN_multihiddenlayer(train_X,train_y,layernodes,iter_num,alpha)\n",
    "ypred_train,activations_train=forward_prop(train_X,final_thetas,n_layers)\n",
    "ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "print(\"Confusion Matrix of training data\\n\")\n",
    "cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "print(cf)\n",
    "pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "auc_train=calc_AUCscore(list(actual_train),list(ypred_labels_train))\n",
    "print(\"Precision : \",str(pr))\n",
    "print(\"Recall : \",str(rc))\n",
    "print(\"F1 score : \",str(f1))\n",
    "print(\"AUC score : \",str(auc_train))\n",
    "plot_costfunction(iter_num,final_cost,'training data')\n",
    "plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the test data with the neural network of single hidden layer and 5 hidden nodes.\n",
    "alpha=0.7\n",
    "layernodes=[4,5,4]\n",
    "n_layers=len(layernodes)\n",
    "print(\"learning_rate :\\n\",str(alpha))\n",
    "ypred_test,activations_test=forward_prop(test_X,final_thetas,n_layers)\n",
    "ypred_labels_test=np.argmax(ypred_test,axis=0)\n",
    "print(\"Confusion Matrix of test data\\n\")\n",
    "cf=confusion_matrix(list(actual_test),list(ypred_labels_test))\n",
    "print(cf)\n",
    "plot_confusion_matrix(cf)\n",
    "pr_test=precision_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "rc_test=recall_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "f1_test=f1_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "auc_test=calc_AUCscore(list(actual_test),list(ypred_labels_test))\n",
    "print(\"Precision : \",str(pr_test))\n",
    "print(\"Recall : \",str(rc_test))\n",
    "print(\"F1 score : \",str(f1_test))\n",
    "print(\"AUC score : \",str(auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single hidden layer with different number of hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the neural network model of single layer with different number of hidden nodes on training data\n",
    "alpha=0.7\n",
    "iter_num=1500\n",
    "final_thetas_list=[]\n",
    "layernodeslist=[[4,2,4],[4,3,4],[4,5,4],[4,7,4],[4,9,4],[4,50,4]]\n",
    "for i in layernodeslist:\n",
    "    print(\"layernode :\",str(i))\n",
    "    print(\"number of nodes in the hidden layer :\",str(i[1]))\n",
    "    layernodes=i\n",
    "    n_layers=len(i)\n",
    "    print(\"learning_rate :\\n\",str(alpha))\n",
    "    final_cost,final_thetas=NN_multihiddenlayer(train_X,train_y,i,iter_num,alpha)\n",
    "    final_thetas_list.append(final_thetas)\n",
    "    ypred_train,activations_train=forward_prop(train_X,final_thetas,n_layers)\n",
    "    ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "    print(cf)\n",
    "    pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    auc_train=calc_AUCscore(list(actual_train),list(ypred_labels_train))\n",
    "    print(\"Precision : \",str(pr))\n",
    "    print(\"Recall : \",str(rc))\n",
    "    print(\"F1 score : \",str(f1))\n",
    "    print(\"AUC score : \",str(auc_train))\n",
    "    plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predicting the test data with  the neural network model of single layer with different number of hidden nodes\n",
    "alpha=0.7\n",
    "\n",
    "layernodeslist=[[4,2,4],[4,3,4],[4,5,4],[4,7,4],[4,9,4],[4,50,4]]\n",
    "for j,i in enumerate(layernodeslist):\n",
    "    print(\"layernode :\",str(i))\n",
    "    print(\"number of nodes in the hidden layer :\",str(i[1]))\n",
    "    layernodes=i\n",
    "    n_layers=len(i)\n",
    "    print(\"learning_rate :\\n\",str(alpha))\n",
    "    final_thetas=final_thetas_list[j]\n",
    "    ypred_test,activations_test=forward_prop(test_X,final_thetas,n_layers)\n",
    "    ypred_labels_test=np.argmax(ypred_test,axis=0)\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    cf=confusion_matrix(list(actual_test),list(ypred_labels_test))\n",
    "    print(cf)\n",
    "    pr_test=precision_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    rc_test=recall_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    f1_test=f1_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    auc_test=calc_AUCscore(list(actual_test),list(ypred_labels_test))\n",
    "    print(\"Precision : \",str(pr_test))\n",
    "    print(\"Recall : \",str(rc_test))\n",
    "    print(\"F1 score : \",str(f1_test))\n",
    "    print(\"AUC score : \",str(auc_test))\n",
    "    plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Applying the neural network model of different number of layers with 5 hidden nodes in each layer on training data\n",
    "alpha=0.7\n",
    "iter_num=1500\n",
    "final_thetas_list=[]\n",
    "layernodeslist=[[4,5,4],[4,5,5,4],[4,5,5,5,4],[4,5,5,5,5,4]]\n",
    "for i in layernodeslist:\n",
    "    print(\"layernode :\",str(i))\n",
    "    \n",
    "    layernodes=i\n",
    "    n_layers=len(i)\n",
    "    print(\"number of layers in the hidden layer :\",str(n_layers-2))\n",
    "    print(\"learning_rate :\\n\",str(alpha))\n",
    "    final_cost,final_thetas=NN_multihiddenlayer(train_X,train_y,i,iter_num,alpha)\n",
    "    final_thetas_list.append(final_thetas)\n",
    "    ypred_train,activations_train=forward_prop(train_X,final_thetas,n_layers)\n",
    "    ypred_labels_train=np.argmax(ypred_train,axis=0)\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    cf=confusion_matrix(list(actual_train),list(ypred_labels_train))\n",
    "    print(cf)\n",
    "    pr=precision_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    rc=recall_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    f1=f1_score(list(actual_train),list(ypred_labels_train),average='weighted')\n",
    "    auc_train=calc_AUCscore(list(actual_train),list(ypred_labels_train))\n",
    "    print(\"Precision : \",str(pr))\n",
    "    print(\"Recall : \",str(rc))\n",
    "    print(\"F1 score : \",str(f1))\n",
    "    print(\"AUC score : \",str(auc_train))\n",
    "    plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predicting the test data by using the  the neural network model of different number of layers with 5 hidden nodes in each layer\n",
    "alpha=0.7\n",
    "layernodeslist=[[4,5,4],[4,5,5,4],[4,5,5,5,4],[4,5,5,5,5,4]]\n",
    "for j,i in enumerate(layernodeslist):\n",
    "    print(\"layernode :\",str(i))\n",
    "    layernodes=i\n",
    "    n_layers=len(i)\n",
    "    print(\"number of layers in the hidden layer :\",str(n_layers-2))\n",
    "    print(\"learning_rate :\\n\",str(alpha))\n",
    "    final_thetas=final_thetas_list[j]\n",
    "    ypred_test,activations_test=forward_prop(test_X,final_thetas,n_layers)\n",
    "    ypred_labels_test=np.argmax(ypred_test,axis=0)\n",
    "    print(\"Confusion Matrix \\n\")\n",
    "    cf=confusion_matrix(list(actual_test),list(ypred_labels_test))\n",
    "    print(cf)\n",
    "    plot_confusion_matrix(cf)\n",
    "    pr_test=precision_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    rc_test=recall_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    f1_test=f1_score(list(actual_test),list(ypred_labels_test),average='weighted')\n",
    "    auc_test=calc_AUCscore(list(actual_test),list(ypred_labels_test))\n",
    "    print(\"Precision : \",str(pr_test))\n",
    "    print(\"Recall : \",str(rc_test))\n",
    "    print(\"F1 score : \",str(f1_test))\n",
    "    print(\"AUC score : \",str(auc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
